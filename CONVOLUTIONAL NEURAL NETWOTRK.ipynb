{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>CONVOLUTIONAL NEURAL NETWOTRK</h1></center>\n",
    "\n",
    "For complete programme open the link üëâüèªhttps://github.com/rajeshrishi/convolutional-neural-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>CLASSIFICATION OF CATS AND DOGS USING CNN</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this CNN model, We'll work on cats and dogs dataset which consists of test and training datasets of differents cats and dogs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "Two types of datasets are included, test set and training set.\n",
    "The test set consisted of 2002 images, 1001 images of Cats and 1001 images of Dogs. \n",
    "<center><h1>Test Set - Cats</h1></center>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/rajeshrishi/convolutional-neural-network/main/download%20(1).jpg\" layout=\"centre\">\n",
    "\n",
    "<center><h1>Test Set - Dogs</h1></center>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/rajeshrishi/convolutional-neural-network/main/download%20(2).jpg\" layout=\"centre\">\n",
    "\n",
    "The training set consisted of 8002 images, 4001 images of Cats and 4001 images of Dogs. \n",
    "<center><h1>Training Set - Cats</h1></center>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/rajeshrishi/convolutional-neural-network/main/download%20(3).jpg\" layout=\"centre\">\n",
    "\n",
    "<center><h1>Training Set - Dogs</h1></center>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/rajeshrishi/convolutional-neural-network/main/download%20(4).jpg\" layout=\"centre\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\rajesh rishi\\anaconda3\\lib\\site-packages (7.0.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing PIL\n",
    "! pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the CNN\n",
    "import keras\n",
    "# Keras is used to import Convolution, Max Pooling, Flattern, Dense and\n",
    "# dropout.\n",
    "import PIL\n",
    "# PIL is a library used for reading or preprocessing the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense, Dropout\n",
    "# Sequential is imported to have data in sequence i.e.,having\n",
    "# Convolution, Max Pooling, Flattern, Dense and dropout in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialiing the CNN\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: convolution\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
    "# In this code \"input_shape=(64, 64, 3)\", \"64,64\" is pixel value and\n",
    "# '3' is used for channel as RGB has 3 channels.\n",
    "# In this code \"Conv2D(32, (3, 3)\", \"(3,3)\" is filter size but matrix\n",
    "# will be random and \"32\" is the number of feature maps and we took 32\n",
    "# because there 64 columns and in ANN the hidden layer should be less\n",
    "# than the input layer (feature maps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: pooling\n",
    "model.add(MaxPooling2D(pool_size= (2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding another convolutional layer to imoprove the accuracy\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: flattening\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: full connection\n",
    "# 1st Hidden layer\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "# 2nd Hidden layer\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# Output Layer\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "# Sigmoid is used or binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 821,409\n",
      "Trainable params: 821,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# 'adam' is a type of optimizer that will decrease our error.\n",
    "# 'binary_crossentropy' is used to measure the error because \n",
    "# we are doing a Classification problem here and the final \n",
    "# result will be either '0' or '1' type.\n",
    "# 'metrics = ['accuracy']' is used to check the accuracy of our model.\n",
    "# 'metrics' is just like confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit CNN to images\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# ImageDataGenerator is a function of Keras which is used for scaling\n",
    "# our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "# In this command we assigned a name \"training_datagen\" and telling \n",
    "# to take this image as a generator and rescale our data.\n",
    "# \"horizontal_flip\" will randomly flip inputs horizontally.\n",
    "# \"zoom_range\" is a range for a random zoom.\n",
    "# \"shear_range\" will shear angle in counter-clockwise direction in degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaling our test data\n",
    "testing_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Reading our training data set\n",
    "training_set = training_datagen.flow_from_directory('dataset/training_set/', \n",
    "                                                    target_size=(64,64), \n",
    "                                                    batch_size=32, \n",
    "                                                    class_mode='binary')\n",
    "# Class is binary because there is only two class i.e., cat and dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Reading our test data set\n",
    "test_set = testing_datagen.flow_from_directory('dataset/test_set/', target_size=(64,64), batch_size=32, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-6085172d98e4>:5: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/25\n",
      "250/250 [==============================] - 236s 945ms/step - loss: 0.6927 - accuracy: 0.5088 - val_loss: 0.6894 - val_accuracy: 0.5320\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 84s 338ms/step - loss: 0.6807 - accuracy: 0.5671 - val_loss: 0.6687 - val_accuracy: 0.5895\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 82s 329ms/step - loss: 0.6627 - accuracy: 0.6086 - val_loss: 0.6401 - val_accuracy: 0.6445\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 81s 324ms/step - loss: 0.6349 - accuracy: 0.6467 - val_loss: 0.6021 - val_accuracy: 0.6750\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 84s 336ms/step - loss: 0.6150 - accuracy: 0.6676 - val_loss: 0.5808 - val_accuracy: 0.6925\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 90s 360ms/step - loss: 0.5868 - accuracy: 0.6975 - val_loss: 0.5480 - val_accuracy: 0.7275\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 83s 331ms/step - loss: 0.5580 - accuracy: 0.7126 - val_loss: 0.5615 - val_accuracy: 0.7060\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 81s 325ms/step - loss: 0.5343 - accuracy: 0.7311 - val_loss: 0.5882 - val_accuracy: 0.7100\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 87s 347ms/step - loss: 0.5235 - accuracy: 0.7471 - val_loss: 0.5776 - val_accuracy: 0.7195\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 83s 332ms/step - loss: 0.5112 - accuracy: 0.7495 - val_loss: 0.4775 - val_accuracy: 0.7835\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 83s 333ms/step - loss: 0.4914 - accuracy: 0.7653 - val_loss: 0.5088 - val_accuracy: 0.7500\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 80s 322ms/step - loss: 0.4828 - accuracy: 0.7706 - val_loss: 0.4721 - val_accuracy: 0.7845\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 81s 325ms/step - loss: 0.4705 - accuracy: 0.7754 - val_loss: 0.4766 - val_accuracy: 0.7805\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 78s 311ms/step - loss: 0.4491 - accuracy: 0.7965 - val_loss: 0.5230 - val_accuracy: 0.7595\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 78s 313ms/step - loss: 0.4472 - accuracy: 0.7937 - val_loss: 0.4877 - val_accuracy: 0.7720\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 80s 322ms/step - loss: 0.4387 - accuracy: 0.7951 - val_loss: 0.4724 - val_accuracy: 0.7785\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 80s 319ms/step - loss: 0.4339 - accuracy: 0.7965 - val_loss: 0.4473 - val_accuracy: 0.8010\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 81s 325ms/step - loss: 0.4257 - accuracy: 0.8033 - val_loss: 0.4456 - val_accuracy: 0.8005\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 81s 325ms/step - loss: 0.4181 - accuracy: 0.8086 - val_loss: 0.4525 - val_accuracy: 0.7985\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 83s 333ms/step - loss: 0.4100 - accuracy: 0.8174 - val_loss: 0.4557 - val_accuracy: 0.7945\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 85s 340ms/step - loss: 0.4062 - accuracy: 0.8155 - val_loss: 0.4595 - val_accuracy: 0.7950\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 84s 335ms/step - loss: 0.3883 - accuracy: 0.8261 - val_loss: 0.4326 - val_accuracy: 0.8165\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 84s 336ms/step - loss: 0.3911 - accuracy: 0.8240 - val_loss: 0.4614 - val_accuracy: 0.7895\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 81s 323ms/step - loss: 0.3767 - accuracy: 0.8317 - val_loss: 0.4436 - val_accuracy: 0.8005\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 81s 324ms/step - loss: 0.3739 - accuracy: 0.8335 - val_loss: 0.4500 - val_accuracy: 0.8025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ef75ffff88>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(training_set, steps_per_epoch=8000/32, epochs=25, validation_data=test_set, validation_steps=2000/32)\n",
    "# In \"steps_per_epoch=8000/32\", 8000is divided by 32 to have 250 batches.\n",
    "# Epoch indicates the number of passes of the entire training dataset \n",
    "# the machine learning algorithm has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "# predictions for dog image using train model\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "tested_image = image.load_img('cat.jpg',target_size=(64,64))\n",
    "tested_image = image.img_to_array(tested_image)\n",
    "tested_image = np.expand_dims(tested_image,axis = 0)\n",
    "output = model.predict(tested_image)\n",
    "training_set.class_indices\n",
    "if output[0][0] ==1:\n",
    "  prediction = 'dog'\n",
    "  print(prediction )\n",
    "else:\n",
    "  prediction = 'cat'\n",
    "  print(prediction )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "tested_image = image.load_img('dog.jpg',target_size=(64,64))\n",
    "tested_image = image.img_to_array(tested_image)\n",
    "tested_image = np.expand_dims(tested_image,axis = 0)\n",
    "output = model.predict(tested_image)\n",
    "training_set.class_indices\n",
    "if output[0][0] ==1:\n",
    "  prediction = 'dog'\n",
    "  print(prediction )\n",
    "else:\n",
    "  prediction = 'cat'\n",
    "  print(prediction )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
